# Bible LLM Tutorial - Lessons

This directory contains interactive lessons for building an LLM from scratch.

## How to Use

Run lessons sequentially:

```bash
uv run python lessons/lesson_01_exploring_data.py
uv run python lessons/lesson_02_tokenization.py
uv run python lessons/lesson_03_embeddings.py
# ... and so on
```

## Lessons Overview

### Part 1: Understanding the Data
- **Lesson 1**: Exploring the Bible Corpus - Load and inspect data
- **Lesson 2**: Tokenization - Convert text to numbers

### Part 2: Building Blocks
- **Lesson 3**: Embeddings - Represent words as vectors
- **Lesson 4**: Attention Mechanism - How models focus
- **Lesson 5**: Multi-Head Attention - Parallel understanding

### Part 3: The Transformer Architecture
- **Lesson 6**: Feed-Forward Networks - Process information
- **Lesson 7**: Layer Normalization - Stabilize training
- **Lesson 8**: Complete Transformer Block - Put it together

### Part 4: Training the Model
- **Lesson 9**: Loss Functions - Measure performance
- **Lesson 10**: Training Loop - Teach the model
- **Lesson 11**: Optimization - Efficient training

### Part 5: Text Generation
- **Lesson 12**: Sampling Strategies - Generate text
- **Lesson 13**: Evaluation - Measure quality

## Tips

- **Be Interactive**: Press Enter to move through sections
- **Experiment**: Modify the code and try your own ideas
- **Take Notes**: Write down observations
- **Ask Questions**: Investigate anything unclear
- **Be Patient**: Building an LLM is complex but rewarding!

## Progress Tracker

Track your progress:
- [ ] Lesson 1: Exploring Data
- [ ] Lesson 2: Tokenization
- [ ] Lesson 3: Embeddings
- [ ] Lesson 4: Attention
- [ ] Lesson 5: Multi-Head Attention
- [ ] Lesson 6: Feed-Forward Networks
- [ ] Lesson 7: Layer Normalization
- [ ] Lesson 8: Transformer Block
- [ ] Lesson 9: Loss Functions
- [ ] Lesson 10: Training Loop
- [ ] Lesson 11: Optimization
- [ ] Lesson 12: Sampling
- [ ] Lesson 13: Evaluation

Good luck! ðŸŽ“
